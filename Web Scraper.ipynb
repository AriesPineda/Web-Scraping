{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f0e69d",
   "metadata": {},
   "source": [
    "# Web Scraper Project\n",
    "## Use to scrape data from a website(Text,URL & Table Data)\n",
    "\n",
    "This small Web Scraper Project is a versatile web scraping tool leveraging Selenium for browser automation and Pandas for data manipulation. Users input the target website's URL, optional XPath for a cookie acceptance button, and the desired number of pages to scrape. The script initializes a Chrome WebDriver, handles cookie acceptance if applicable, and manages CSV file creation and encoding detection. Users can choose between scraping individual page elements or entire tables, with the script dynamically adapting to user-defined XPaths and providing summary reports of the scraping process. This interactive tool empowers users to efficiently collect structured data from websites, making it adaptable for a range of web scraping scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc777ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import chardet\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        result = chardet.detect(file.read())\n",
    "    return result['encoding']\n",
    "\n",
    "def read_file(file_path, encoding):\n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "def create_csv_file(file_path):\n",
    "    with open(file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        pass  # Just create an empty file\n",
    "\n",
    "def get_element_data():\n",
    "    element_data = []\n",
    "    while True:\n",
    "        elements_data = []\n",
    "        while True:\n",
    "            element_name = input(\"Enter the name of the element (e.g., 'titles', 'location', 'salary', 'contract_type', 'job_details') or type 'q' to finish: \")\n",
    "            if element_name == 'q':\n",
    "                break\n",
    "            selector_type = int(input(f\"Enter the selector type for the '{element_name}' element:\\n1 for XPath\\n2 for CSS selector: \"))\n",
    "            selector = input(f\"Enter the selector for the '{element_name}' element: \")\n",
    "            data_type = int(input(f\"Enter the data type for the '{element_name}' element:\\n1 for text\\n2 for URL: \"))\n",
    "            elements_data.append((element_name, selector_type, selector, data_type))\n",
    "\n",
    "        if not elements_data:\n",
    "            break\n",
    "\n",
    "        element_data.extend(elements_data)\n",
    "\n",
    "    return element_data\n",
    "\n",
    "def scrape_table(table_selector, csv_filename):\n",
    "    try:\n",
    "        table = driver.find_element(\"xpath\", table_selector)\n",
    "        header_row = table.find_element(\"xpath\", \".//tr[1]\")\n",
    "        header_cells = header_row.find_elements(\"xpath\", \".//th\")\n",
    "        header_data = [cell.text for cell in header_cells]\n",
    "\n",
    "        with open(csv_filename, 'w', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "\n",
    "            # Write the table headers to the CSV file\n",
    "            csv_writer.writerow(header_data)\n",
    "\n",
    "            rows = table.find_elements(\"xpath\", \".//tr[position()>1]\")\n",
    "            for row in rows:\n",
    "                cells = row.find_elements(\"xpath\", \".//td\")\n",
    "                row_data = [cell.text for cell in cells]\n",
    "                csv_writer.writerow(row_data)\n",
    "        print(f\"Table data and headers have been scraped and saved to '{csv_filename}'\")\n",
    "    except NoSuchElementException:\n",
    "        print(\"Table not found on the page.\")\n",
    "        \n",
    "        \n",
    "def write_element_data_to_csv(element_data, csv_filename):\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "\n",
    "        element_names = [element[0] for element in element_data]\n",
    "        csv_writer.writerow(element_names)\n",
    "\n",
    "        for _ in range(page_count):\n",
    "            data = []\n",
    "            for element_name, selector_type, selector, data_type in element_data:\n",
    "                if selector_type == 1:\n",
    "                    elements = driver.find_elements(\"xpath\", selector)\n",
    "                elif selector_type == 2:\n",
    "                    elements = driver.find_elements(\"css selector\", selector)\n",
    "\n",
    "                if data_type == 1:\n",
    "                    data.append([element.text for element in elements])\n",
    "                elif data_type == 2:\n",
    "                    data.append([element.get_attribute('href') if element.tag_name == 'a' else '' for element in elements])\n",
    "\n",
    "            for row in zip(*data):\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "            try:\n",
    "                if page_count != 1:\n",
    "                    if next_button_selector_type == 1:\n",
    "                        next = driver.find_element(\"xpath\", next_button_selector)\n",
    "                    elif next_button_selector_type == 2:\n",
    "                        next = driver.find_element(\"css selector\", next_button_selector)\n",
    "\n",
    "                    next.click()\n",
    "\n",
    "                    time.sleep(5)\n",
    "            except NoSuchElementException:\n",
    "                break\n",
    "\n",
    "    print(\"Scraping elements completed successfully.\")\n",
    "\n",
    "# Create a WebDriver instance\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "url = input(\"Enter the URL of the website you want to scrape: \")\n",
    "driver.get(url)\n",
    "cookie_xpath = input(\"Enter the XPath of the cookie acceptance button or press 'q' to skip: \")\n",
    "\n",
    "if cookie_xpath.lower() != 'q':\n",
    "    try:\n",
    "        cookie_button = driver.find_element(\"xpath\", cookie_xpath)\n",
    "        cookie_button.click()\n",
    "        time.sleep(5)\n",
    "    except NoSuchElementException:\n",
    "        print(\"Cookie acceptance button not found. Skipping...\")\n",
    "\n",
    "page_count = int(input(\"Enter the number of pages to scrape: \"))\n",
    "if page_count == 1:\n",
    "    print(\"Only 1 page to scrape. Skipping the 'Next' button identifiers.\")\n",
    "\n",
    "# Specify the name of the CSV file\n",
    "csv_filename = input(\"Enter the name of the CSV file (e.g., 'output.csv'): \")\n",
    "\n",
    "# Create the CSV file if it doesn't exist\n",
    "if not os.path.exists(csv_filename):\n",
    "    create_csv_file(csv_filename)\n",
    "\n",
    "# Detect encoding of the file\n",
    "file_encoding = detect_encoding(csv_filename)\n",
    "\n",
    "# Read the file using the detected encoding\n",
    "file_content = read_file(csv_filename, file_encoding)\n",
    "\n",
    "summary_report_filename = \"summary_report.csv\"\n",
    "main_csv_name = csv_filename\n",
    "\n",
    "summary_report_exists = os.path.isfile(summary_report_filename)\n",
    "write_mode = 'w' if not summary_report_exists else 'a'\n",
    "\n",
    "while True:\n",
    "    print(\"Choose an option:\")\n",
    "    print(\"1 for scraping elements\")\n",
    "    print(\"2 for scraping a table\")\n",
    "    print(\"3 to Quit\")\n",
    "    option = input(\"Enter your choice: \")\n",
    "    if option == '1':\n",
    "        element_data = get_element_data()\n",
    "\n",
    "        if page_count == 1:\n",
    "            next_button_selector_type = 0\n",
    "            next_button_selector = \"\"\n",
    "    \n",
    "        else:\n",
    "            next_button_selector_type = int(input(\"Enter the selector type for the 'Next' button:\\n1 for XPath\\n2 for CSS selector: \"))\n",
    "            next_button_selector = input(\"Enter the selector of the 'Next' button to navigate to the next page: \")\n",
    "\n",
    "        write_element_data_to_csv(element_data, csv_filename)\n",
    "\n",
    "        df = pd.read_csv(csv_filename, encoding='utf-8')  # Explicitly specify encoding\n",
    "        page_count = page_count if page_count != 0 else 1\n",
    "        column_count = len(df.columns)\n",
    "        row_count = len(df) + 1\n",
    "        total_data_count = len(df.values.ravel())\n",
    "        now = datetime.now()\n",
    "        current_datetime = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        current_summary = pd.DataFrame({\n",
    "            'Date and Time': [current_datetime],\n",
    "            'Website': [url],\n",
    "            'Main CSV Name': [main_csv_name],\n",
    "            'Number of Pages': [page_count],\n",
    "            'Column Count': [column_count],\n",
    "            'Row Count': [row_count],\n",
    "            'Total Data Count': [total_data_count]\n",
    "        })\n",
    "\n",
    "        current_summary.to_csv(summary_report_filename, mode=write_mode, header=not summary_report_exists, index=False)\n",
    "\n",
    "        if not summary_report_exists:\n",
    "            print(f\"'{summary_report_filename}' file has been created with headers.\")\n",
    "        else:\n",
    "            print(\"Summary report has been successfully updated.\")\n",
    "\n",
    "    elif option == '2':\n",
    "        # Ask the user for the table selector and file name\n",
    "        table_selector = input(\"Enter the XPath of the table you want to scrape: \")\n",
    "        scrape_table(table_selector, csv_filename)\n",
    "        print(\"Scraping table completed successfully.\")\n",
    "\n",
    "        df = pd.read_csv(csv_filename, encoding='utf-8')\n",
    "        page_count = page_count if page_count != 0 else 1\n",
    "        column_count = len(df.columns)\n",
    "        row_count = len(df) + 1\n",
    "        total_data_count = len(df.values.ravel())\n",
    "        now = datetime.now()\n",
    "        current_datetime = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        current_summary = pd.DataFrame({\n",
    "            'Date and Time': [current_datetime],\n",
    "            'Website': [url],\n",
    "            'Main CSV Name': [main_csv_name],\n",
    "            'Number of Pages': [page_count],\n",
    "            'Column Count': [column_count],\n",
    "            'Row Count': [row_count],\n",
    "            'Total Data Count': [total_data_count]\n",
    "        })\n",
    "\n",
    "        current_summary.to_csv(summary_report_filename, mode=write_mode, header=not summary_report_exists, index=False)\n",
    "\n",
    "        if not summary_report_exists:\n",
    "            print(f\"'{summary_report_filename}' file has been created with headers.\")\n",
    "        else:\n",
    "            print(\"Summary report has been successfully updated.\")\n",
    "\n",
    "    elif option == '3':\n",
    "        break\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839e9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
